{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7AKQG1e91tiZ","AppdIqSp1pvf","KZQumzst2Co4","F4lzQ2Au24po","lTW5M2tX4jz7","VfXf0ZziF6CB","d9zudUyMIJWb","7lOhUv0WCFTg"],"authorship_tag":"ABX9TyMryFChpeLqmLagk2t5/dHK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Install Needed Dependencies"],"metadata":{"id":"7AKQG1e91tiZ"}},{"cell_type":"code","source":["!pip install pytorch-metric-learning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Y7NF5-Ph10ai","executionInfo":{"status":"ok","timestamp":1749311675525,"user_tz":-180,"elapsed":111995,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"6ecf103b-3841-44e8-e6b4-9bf6dd71751a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-metric-learning\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.6.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (3.0.2)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-metric-learning\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-metric-learning-2.8.1\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision pyyaml scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bsdMMg2l12wt","executionInfo":{"status":"ok","timestamp":1749311686991,"user_tz":-180,"elapsed":7707,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"953aaf10-3c48-4c19-944b-28ec27ab22ac"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"markdown","source":["# Load Dataset"],"metadata":{"id":"AppdIqSp1pvf"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"pecjRV871NzX","executionInfo":{"status":"ok","timestamp":1749311690940,"user_tz":-180,"elapsed":16,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["import os\n","\n","os.makedirs('/content/dataset_folder', exist_ok=True)"]},{"cell_type":"code","source":["!pip install -U gdown\n","\n","file_id = '1iVnP4gjw-iHXa0KerZQ1IfIO0i1jADsR'\n","output_file = '/content/dataset.zip'\n","!gdown --id {file_id} --output {output_file}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"y3aZM_aU1ah1","executionInfo":{"status":"ok","timestamp":1749311800784,"user_tz":-180,"elapsed":106126,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"b5420e4f-7c5e-4e5a-b0f9-3fc386c988c8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1iVnP4gjw-iHXa0KerZQ1IfIO0i1jADsR\n","From (redirected): https://drive.google.com/uc?id=1iVnP4gjw-iHXa0KerZQ1IfIO0i1jADsR&confirm=t&uuid=8a0cf342-41e1-4840-b4ea-8b0cc8a2259b\n","To: /content/dataset.zip\n","100% 9.20G/9.20G [01:34<00:00, 97.8MB/s]\n"]}]},{"cell_type":"code","source":["unzip_folder = '/content/dataset_folder'\n","!unzip -q {output_file} -d {unzip_folder}\n","\n","print(\"Contents of dataset_folder:\")\n","print(os.listdir('/content/dataset_folder'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQn95n3x1hMj","executionInfo":{"status":"ok","timestamp":1749311935434,"user_tz":-180,"elapsed":128774,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"2b256301-75f1-4f5d-b043-8e3df72840d7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Contents of dataset_folder:\n","['University-Release']\n"]}]},{"cell_type":"markdown","source":["# ACMMM23-Solution-MBEG"],"metadata":{"id":"KZQumzst2Co4"}},{"cell_type":"code","source":["!git clone https://github.com/Reza-Zhu/ACMMM23-Solution-MBEG.git /content/ACMMM23-Solution-MBEG\n","\n","%cd /content/ACMMM23-Solution-MBEG"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKkxSVNh1-9z","executionInfo":{"status":"ok","timestamp":1749311949974,"user_tz":-180,"elapsed":1433,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"7e13cc14-5e69-4808-ed8f-49632ee2673b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/ACMMM23-Solution-MBEG'...\n","remote: Enumerating objects: 45, done.\u001b[K\n","remote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects: 100% (40/40), done.\u001b[K\n","remote: Total 45 (delta 17), reused 20 (delta 4), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (45/45), 8.80 MiB | 17.70 MiB/s, done.\n","Resolving deltas: 100% (17/17), done.\n","/content/ACMMM23-Solution-MBEG\n"]}]},{"cell_type":"code","source":["weights_path = \"/content/ACMMM23-Solution-MBEG/weights\"\n","\n","os.makedirs(weights_path, exist_ok=True)\n","\n","print(f\"Directory created at: {weights_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QwwPxcE2HOo","executionInfo":{"status":"ok","timestamp":1749311952354,"user_tz":-180,"elapsed":58,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"9ba289a2-7499-4877-b954-dd117285b100"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Directory created at: /content/ACMMM23-Solution-MBEG/weights\n"]}]},{"cell_type":"code","source":["import yaml\n","\n","with open(\"/content/ACMMM23-Solution-MBEG/settings.yaml\", 'r') as f:\n","    config = yaml.safe_load(f)\n","\n","    config['dataset_path'] = '/content/dataset_folder/University-Release'\n","    config['weight_save_path'] = weights_path\n","    config['num_epochs'] = 10\n","    config['batch_size'] = 4\n","    config['model'] = 'ResNet'\n","    config['name'] = 'ResNet_1652'\n","\n","    with open(\"settings.yaml\", 'w') as f:\n","        yaml.dump(config, f)"],"metadata":{"id":"2SGXQrHF2OpH","executionInfo":{"status":"ok","timestamp":1749311955677,"user_tz":-180,"elapsed":105,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/ACMMM23-Solution-MBEG\")"],"metadata":{"id":"9hMcomIx2SYx","executionInfo":{"status":"ok","timestamp":1749311959768,"user_tz":-180,"elapsed":17,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Create a subset training data of 100 classes each containing 20 images"],"metadata":{"id":"F4lzQ2Au24po"}},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","full_dataset_path = \"/content/dataset_folder/University-Release/train/\"\n","subset_path = \"/content/dataset_subset\"\n","subset_size_per_class = 20\n","num_classes_to_sample = 100\n","\n","modalities = ['drone', 'satellite']\n","\n","sat_classes = set(os.listdir(os.path.join(full_dataset_path, 'satellite')))\n","drone_classes = set(os.listdir(os.path.join(full_dataset_path, 'drone')))\n","shared_classes = list(sat_classes & drone_classes)\n","\n","selected_classes = random.sample(shared_classes, min(num_classes_to_sample, len(shared_classes)))\n","print(f\"Selected {len(selected_classes)} classes.\")\n","\n","for modality in modalities:\n","    modality_full_path = os.path.join(full_dataset_path, modality)\n","    modality_subset_path = os.path.join(subset_path, modality)\n","    os.makedirs(modality_subset_path, exist_ok=True)\n","\n","    for cls in selected_classes:\n","        class_full_path = os.path.join(modality_full_path, cls)\n","        class_subset_path = os.path.join(modality_subset_path, cls)\n","        os.makedirs(class_subset_path, exist_ok=True)\n","\n","        all_images = [f for f in os.listdir(class_full_path) if f.lower().endswith(('.jpg','.png','.jpeg'))]\n","        if not all_images:\n","            print(f\"No images found in {class_full_path}, skipping.\")\n","            continue\n","\n","        sampled_images = random.sample(all_images, min(subset_size_per_class, len(all_images)))\n","\n","        for img_name in sampled_images:\n","            src = os.path.join(class_full_path, img_name)\n","            dst = os.path.join(class_subset_path, img_name)\n","            shutil.copyfile(src, dst)\n","\n","print(\"Subset folder with randomly selected classes and images created successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0aCLQkf3BPv","executionInfo":{"status":"ok","timestamp":1749312340955,"user_tz":-180,"elapsed":6524,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"3f726676-b705-4a58-933c-f29154245e31"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected 100 classes.\n","Subset folder with randomly selected classes and images created successfully!\n"]}]},{"cell_type":"markdown","source":["# Modifying Data Preprocessing & Model Definition"],"metadata":{"id":"lTW5M2tX4jz7"}},{"cell_type":"code","source":["code='''\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from PIL import Image\n","from pathlib import Path\n","\n","class DualResNet(nn.Module):\n","    def __init__(self, num_classes=1652, pretrained=True):\n","        super(DualResNet, self).__init__()\n","        self.backbone1 = models.resnet18(pretrained=pretrained)\n","        self.backbone2 = models.resnet18(pretrained=pretrained)\n","        self.backbone1.fc = nn.Identity()\n","        self.backbone2.fc = nn.Identity()\n","        self.classifier1 = nn.Linear(512, num_classes)\n","        self.classifier2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, x1, x2):\n","        f1 = self.backbone1(x1)\n","        f2 = self.backbone2(x2)\n","        out1 = self.classifier1(f1)\n","        out2 = self.classifier2(f2)\n","        return out1, out2, f1, f2\n","\n","def get_num_classes(data_dir):\n","      sat_dir = Path(data_dir) / \"satellite\"\n","      class_folders = [f for f in sat_dir.iterdir() if f.is_dir()]\n","      return len(class_folders)\n","'''\n","with open(\"dualresnet.py\", \"w\") as f:\n","        f.write(code)"],"metadata":{"id":"nbjdx0Ve4nfy","executionInfo":{"status":"ok","timestamp":1749312348934,"user_tz":-180,"elapsed":49,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["code = '''\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from pathlib import Path\n","\n","class PairedU1652Dataset(Dataset):\n","    def __init__(self, root_dir, transform_sat=None, transform_drone=None):\n","        self.root_dir = root_dir\n","\n","        self.sat_dir = os.path.join(root_dir, 'satellite')\n","        self.drone_dir = os.path.join(root_dir, 'drone')\n","\n","        self.transform_sat = transform_sat\n","        self.transform_drone = transform_drone\n","\n","        self.sat_classes = sorted(os.listdir(self.sat_dir))\n","        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.sat_classes)}\n","\n","        self.sat_images = []\n","        self.drone_images = []\n","        self.labels = []\n","\n","        for cls_name in self.sat_classes:\n","            sat_cls_path = os.path.join(self.sat_dir, cls_name)\n","            drone_cls_path = os.path.join(self.drone_dir, cls_name)\n","\n","            sat_imgs = sorted(os.listdir(sat_cls_path))\n","            drone_imgs = sorted(os.listdir(drone_cls_path))\n","\n","            assert len(sat_imgs) == 1, f\"Expected exactly 1 satellite image per class '{cls_name}', but got {len(sat_imgs)}\"\n","\n","            sat_img_name = sat_imgs[0]\n","            sat_img_path = os.path.join(sat_cls_path, sat_img_name)\n","\n","            for drone_img_name in drone_imgs:\n","                drone_img_path = os.path.join(drone_cls_path, drone_img_name)\n","\n","                self.sat_images.append(sat_img_path)       # Repeat the same satellite image path\n","                self.drone_images.append(drone_img_path)\n","                self.labels.append(self.class_to_idx[cls_name])\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        sat_img_path = self.sat_images[idx]\n","        drone_img_path = self.drone_images[idx]\n","        label = self.labels[idx]\n","\n","        sat_img = Image.open(sat_img_path).convert('RGB')\n","        drone_img = Image.open(drone_img_path).convert('RGB')\n","\n","        if self.transform_sat:\n","            sat_img = self.transform_sat(sat_img)\n","        if self.transform_drone:\n","            drone_img = self.transform_drone(drone_img)\n","\n","        return sat_img, drone_img, label\n","\n","'''\n","with open(\"Modified_Preprocessing.py\", \"w\") as f:\n","    f.write(code)\n"],"metadata":{"id":"vcMm0hN8KsGf","executionInfo":{"status":"ok","timestamp":1749314704692,"user_tz":-180,"elapsed":59,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["import importlib\n","import Modified_Preprocessing\n","importlib.reload(Modified_Preprocessing)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BshE8wtXQZTO","executionInfo":{"status":"ok","timestamp":1749314707152,"user_tz":-180,"elapsed":85,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"5fd74882-d790-4242-ea38-c5e6e53d3a77"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'Modified_Preprocessing' from '/content/ACMMM23-Solution-MBEG/Modified_Preprocessing.py'>"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["# Train Using DualResNet"],"metadata":{"id":"VfXf0ZziF6CB"}},{"cell_type":"code","source":["import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","from dualresnet import DualResNet\n","from Modified_Preprocessing import PairedU1652Dataset, get_num_classes\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","\n","def train():\n","    num_epochs = get_yaml_value(\"num_epochs\")\n","    lr = get_yaml_value(\"lr\")\n","    batch_size = get_yaml_value(\"batch_size\")\n","    data_dir = \"/content/dataset_subset\"\n","    device = torch.device(\"cpu\")\n","\n","    num_classes = 100\n","    print(f\"\\n Detected number of classes: {num_classes}\")\n","\n","    transform_sat = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","    transform_drone = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","\n","    dataset = PairedU1652Dataset(root_dir=data_dir,\n","                                transform_sat=transform_sat, transform_drone=transform_drone)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","    model = DualResNet(num_classes=num_classes).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    best_acc = 0.0\n","    weights_dir = \"/content/ACMMM23-Solution-MBEG/weights\"\n","    print(\"\\n Starting training...\\n\")\n","    since = time.time()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        correct1 = correct2 = total = 0\n","\n","        for sat_img, drone_img, labels in dataloader:\n","            sat_img, drone_img, labels = sat_img.to(device), drone_img.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            out1, out2, _, _ = model(sat_img, drone_img)\n","\n","            loss1 = criterion(out1, labels)\n","            loss2 = criterion(out2, labels)\n","            loss = (loss1 + loss2) / 2\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * labels.size(0)\n","            correct1 += (out1.argmax(1) == labels).sum().item()\n","            correct2 += (out2.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        epoch_loss = total_loss / total\n","        acc1 = correct1 / total\n","        acc2 = correct2 / total\n","        avg_acc = (acc1 + acc2) / 2\n","\n","        print(f\" Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Sat Acc: {acc1:.4f}, Drone Acc: {acc2:.4f}\")\n","\n","        if avg_acc > best_acc:\n","            best_acc = avg_acc\n","            weights_path = os.path.join(weights_dir, \"best_model.pth\")\n","            torch.save(model.state_dict(), weights_path)\n","            print(f\"New best model saved with avg accuracy: {best_acc:.4f}\")\n","\n","    time_elapsed = time.time() - since\n","    print(f\"\\nTraining complete in {int(time_elapsed // 60)}m {int(time_elapsed % 60)}s\")\n","    print(f\" Best avg accuracy: {best_acc:.4f}\")\n","if __name__ == '__main__':\n","    train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fiXU4-JBmLa","executionInfo":{"status":"ok","timestamp":1749327465961,"user_tz":-180,"elapsed":11348376,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"1223017d-c274-47b8-ff6b-fa606c893761"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Detected number of classes: 100\n","\n"," Starting training...\n","\n"," Epoch [1/10] | Loss: 5.0207 | Sat Acc: 0.0200, Drone Acc: 0.0170\n","New best model saved with avg accuracy: 0.0185\n"," Epoch [2/10] | Loss: 4.0769 | Sat Acc: 0.0810, Drone Acc: 0.0335\n","New best model saved with avg accuracy: 0.0573\n"," Epoch [3/10] | Loss: 3.3876 | Sat Acc: 0.2695, Drone Acc: 0.0550\n","New best model saved with avg accuracy: 0.1623\n"," Epoch [4/10] | Loss: 2.3824 | Sat Acc: 0.7440, Drone Acc: 0.0715\n","New best model saved with avg accuracy: 0.4078\n"," Epoch [5/10] | Loss: 1.8651 | Sat Acc: 0.9630, Drone Acc: 0.0995\n","New best model saved with avg accuracy: 0.5312\n"," Epoch [6/10] | Loss: 1.6415 | Sat Acc: 0.9955, Drone Acc: 0.1455\n","New best model saved with avg accuracy: 0.5705\n"," Epoch [7/10] | Loss: 1.4621 | Sat Acc: 0.9770, Drone Acc: 0.2335\n","New best model saved with avg accuracy: 0.6052\n"," Epoch [8/10] | Loss: 1.2339 | Sat Acc: 0.9875, Drone Acc: 0.3275\n","New best model saved with avg accuracy: 0.6575\n"," Epoch [9/10] | Loss: 1.0106 | Sat Acc: 0.9870, Drone Acc: 0.4215\n","New best model saved with avg accuracy: 0.7043\n"," Epoch [10/10] | Loss: 0.8401 | Sat Acc: 0.9760, Drone Acc: 0.5315\n","New best model saved with avg accuracy: 0.7537\n","\n","Training complete in 189m 7s\n"," Best avg accuracy: 0.7537\n"]}]},{"cell_type":"code","source":["code = '''\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","from dualresnet import DualResNet\n","from Modified_Preprocessing import PairedU1652Dataset, get_num_classes\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","\n","def train():\n","    # Configs\n","    num_epochs = get_yaml_value(\"num_epochs\")\n","    lr = get_yaml_value(\"lr\")\n","    batch_size = get_yaml_value(\"batch_size\")\n","    data_dir = \"/content/dataset_subset\"\n","    device = torch.device(\"cpu\")\n","\n","    #sat_dir = '/content/dataset_subset/satellite'\n","    #num_classes = get_num_classes(sat_dir)\n","    num_classes = 100\n","    print(f\"\\n Detected number of classes: {num_classes}\")\n","\n","    transform_sat = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","    transform_drone = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","\n","    dataset = PairedU1652Dataset(root_dir=data_dir,\n","                                transform_sat=transform_sat, transform_drone=transform_drone)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","    model = DualResNet(num_classes=num_classes).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    best_acc = 0.0\n","    weights_dir = \"/content/ACMMM23-Solution-MBEG/weights\"\n","    print(\"\\n Starting training...\\n\")\n","    since = time.time()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        correct1 = correct2 = total = 0\n","\n","        for sat_img, drone_img, labels in dataloader:\n","            sat_img, drone_img, labels = sat_img.to(device), drone_img.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            out1, out2, _, _ = model(sat_img, drone_img)\n","\n","            loss1 = criterion(out1, labels)\n","            loss2 = criterion(out2, labels)\n","            loss = (loss1 + loss2) / 2\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * labels.size(0)\n","            correct1 += (out1.argmax(1) == labels).sum().item()\n","            correct2 += (out2.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        epoch_loss = total_loss / total\n","        acc1 = correct1 / total\n","        acc2 = correct2 / total\n","        avg_acc = (acc1 + acc2) / 2\n","\n","        print(f\" Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Sat Acc: {acc1:.4f}, Drone Acc: {acc2:.4f}\")\n","\n","        if avg_acc > best_acc:\n","            best_acc = avg_acc\n","            weights_path = os.path.join(weights_dir, \"best_model.pth\")\n","            torch.save(model.state_dict(), weights_path)\n","            print(f\"New best model saved with avg accuracy: {best_acc:.4f}\")\n","\n","    time_elapsed = time.time() - since\n","    print(f\"\\nTraining complete in {int(time_elapsed // 60)}m {int(time_elapsed % 60)}s\")\n","    print(f\" Best avg accuracy: {best_acc:.4f}\")\n","if __name__ == '__main__':\n","    train()\n","'''\n","with open(\"train.py\", \"w\") as f:\n","      f.write(code)"],"metadata":{"id":"7DJobxBlCLv0","executionInfo":{"status":"ok","timestamp":1749327487896,"user_tz":-180,"elapsed":67,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["# Create subset of testing dataset of 100 classes"],"metadata":{"id":"d9zudUyMIJWb"}},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","full_dataset_path = \"/content/dataset_folder/University-Release/test/\"\n","subset_path = \"/content/dataset_subset_test\"\n","num_classes_to_sample = 100\n","\n","modalities = ['query_drone', 'query_satellite']\n","\n","\n","sat_classes = set(os.listdir(os.path.join(full_dataset_path, 'query_satellite')))\n","drone_classes = set(os.listdir(os.path.join(full_dataset_path, 'query_drone')))\n","shared_classes = list(sat_classes & drone_classes)\n","\n","\n","selected_classes = random.sample(shared_classes, min(num_classes_to_sample, len(shared_classes)))\n","print(f\"Selected {len(selected_classes)} classes.\")\n","\n","\n","for modality in modalities:\n","    modality_full_path = os.path.join(full_dataset_path, modality)\n","    modality_subset_path = os.path.join(subset_path, modality)\n","    os.makedirs(modality_subset_path, exist_ok=True)\n","\n","    for cls in selected_classes:\n","        class_full_path = os.path.join(modality_full_path, cls)\n","        class_subset_path = os.path.join(modality_subset_path, cls)\n","        os.makedirs(class_subset_path, exist_ok=True)\n","\n","        all_images = [f for f in os.listdir(class_full_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n","        if not all_images:\n","            print(f\"No images found in {class_full_path}, skipping.\")\n","            continue\n","\n","\n","        for img_name in all_images:\n","            src = os.path.join(class_full_path, img_name)\n","            dst = os.path.join(class_subset_path, img_name)\n","            shutil.copyfile(src, dst)\n","\n","print(\"Subset folder with all images from selected classes created successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQiCwTCNTu5z","executionInfo":{"status":"ok","timestamp":1749332188488,"user_tz":-180,"elapsed":7578,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"d2157c5b-7d1a-4ebc-a279-b9ea00972334"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected 100 classes.\n","Subset folder with all images from selected classes created successfully!\n"]}]},{"cell_type":"markdown","source":["# Testing & Evaluating"],"metadata":{"id":"7lOhUv0WCFTg"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from dualresnet import DualResNet\n","\n","\n","def make_dataloaders(root_dir, image_size=224, batch_size=16, num_workers=2):\n","    transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","    ds_sat = datasets.ImageFolder(os.path.join(root_dir, \"query_satellite\"), transform)\n","    ds_dr = datasets.ImageFolder(os.path.join(root_dir, \"query_drone\"), transform)\n","\n","    return {\n","        'satellite': DataLoader(ds_sat, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n","        'drone': DataLoader(ds_dr, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n","    }, {\n","        'satellite': ds_sat,\n","        'drone': ds_dr\n","    }\n","\n","import numpy as np\n","import torch\n","\n","def compute_mAP(index, good_index, junk_index):\n","    cmc = torch.IntTensor(len(index)).zero_()\n","    if len(good_index) == 0:\n","        return 0.0, cmc\n","\n","    mask = ~np.isin(index, junk_index)\n","    index = index[mask]\n","\n","    order = np.where(np.isin(index, good_index))[0]\n","\n","    if len(order) == 0:\n","        return 0.0, cmc\n","\n","    cmc[order[0]:] = 1\n","\n","    num_good = len(good_index)\n","    precision_at_i = [(i + 1) / (rank + 1) for i, rank in enumerate(order)]\n","    ap = np.sum(precision_at_i) / num_good\n","\n","    return ap, cmc\n","\n","def evaluate(qf, ql, gf, gl):\n","    qf = qf.view(1, -1)\n","    scores = torch.nn.functional.cosine_similarity(gf, qf, dim=1).cpu().numpy()\n","    index = np.argsort(scores)[::-1]\n","    gl = np.asarray(gl)\n","\n","    good_index = np.argwhere(gl == ql).flatten()\n","    junk_index = np.argwhere(gl == -1).flatten()\n","\n","    ap, cmc = compute_mAP(index, good_index, junk_index)\n","    return ap, cmc\n","\n","\n","def extract_features(model, loader, view=1):\n","    model.eval()\n","    feats, labels = [], []\n","    with torch.no_grad():\n","        for imgs, labs in loader:\n","            if view == 1:\n","                feat, _, _, _ = model(imgs, torch.zeros_like(imgs))\n","            else:\n","                _, feat, _, _ = model(torch.zeros_like(imgs), imgs)\n","            feat = F.normalize(feat, dim=1)\n","            feats.append(feat.cpu())\n","            labels.extend(labs.numpy())\n","    return torch.cat(feats), np.array(labels)\n","\n","\n","def test():\n","    root_dir = '/content/dataset_subset_test'\n","    weights_path = '/content/ACMMM23-Solution-MBEG/weights/best_model.pth'\n","\n","    dataloaders, datasets = make_dataloaders(root_dir)\n","    model = DualResNet(num_classes=len(datasets['satellite'].classes))\n","    model.load_state_dict(torch.load(weights_path, map_location='cpu'))\n","    model = model.to('cpu')\n","    model.eval()\n","\n","    gf, gl = extract_features(model, dataloaders['drone'], view=2)\n","    qf, ql = extract_features(model, dataloaders['satellite'], view=1)\n","\n","    CMCs, APs = [], []\n","    for i in range(len(ql)):\n","        ap, cmc = evaluate(qf[i], ql[i], gf, gl)\n","        if cmc[0] == -1:\n","            continue\n","        CMCs.append(cmc.unsqueeze(0))\n","        APs.append(ap)\n","\n","    CMC = torch.cat(CMCs).float().mean(0).numpy()\n","    mAP = np.mean(APs)\n","    print(f\"Recall@1: {CMC[0]*100:.2f}%, Recall@5: {CMC[4]*100:.2f}%, mAP: {mAP*100:.2f}%\")\n","\n","\n","if __name__ == '__main__':\n","    test()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLqyY71bQ9t7","executionInfo":{"status":"ok","timestamp":1749333138134,"user_tz":-180,"elapsed":946388,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"6f7d4176-2a34-4c3e-de85-8b04d18ddc5c"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall@1: 2.00%, Recall@5: 7.00%, mAP: 2.68%\n"]}]},{"cell_type":"code","source":["code = '''\n","import os\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from dualresnet import DualResNet\n","\n","\n","def make_dataloaders(root_dir, image_size=224, batch_size=16, num_workers=2):\n","    transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","    ds_sat = datasets.ImageFolder(os.path.join(root_dir, \"query_satellite\"), transform)\n","    ds_dr = datasets.ImageFolder(os.path.join(root_dir, \"query_drone\"), transform)\n","\n","    return {\n","        'satellite': DataLoader(ds_sat, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n","        'drone': DataLoader(ds_dr, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n","    }, {\n","        'satellite': ds_sat,\n","        'drone': ds_dr\n","    }\n","\n","import numpy as np\n","import torch\n","\n","def compute_mAP(index, good_index, junk_index):\n","    cmc = torch.IntTensor(len(index)).zero_()\n","    if len(good_index) == 0:\n","        return 0.0, cmc\n","\n","    mask = ~np.isin(index, junk_index)\n","    index = index[mask]\n","\n","    order = np.where(np.isin(index, good_index))[0]\n","\n","    if len(order) == 0:\n","        return 0.0, cmc\n","\n","    cmc[order[0]:] = 1\n","\n","    num_good = len(good_index)\n","    precision_at_i = [(i + 1) / (rank + 1) for i, rank in enumerate(order)]\n","    ap = np.sum(precision_at_i) / num_good\n","\n","    return ap, cmc\n","\n","def evaluate(qf, ql, gf, gl):\n","    qf = qf.view(1, -1)\n","    scores = torch.nn.functional.cosine_similarity(gf, qf, dim=1).cpu().numpy()\n","    index = np.argsort(scores)[::-1]\n","    gl = np.asarray(gl)\n","\n","    good_index = np.argwhere(gl == ql).flatten()\n","    junk_index = np.argwhere(gl == -1).flatten()\n","\n","    ap, cmc = compute_mAP(index, good_index, junk_index)\n","    return ap, cmc\n","\n","\n","def extract_features(model, loader, view=1):\n","    model.eval()\n","    feats, labels = [], []\n","    with torch.no_grad():\n","        for imgs, labs in loader:\n","            if view == 1:\n","                feat, _, _, _ = model(imgs, torch.zeros_like(imgs))\n","            else:\n","                _, feat, _, _ = model(torch.zeros_like(imgs), imgs)\n","            feat = F.normalize(feat, dim=1)\n","            feats.append(feat.cpu())\n","            labels.extend(labs.numpy())\n","    return torch.cat(feats), np.array(labels)\n","\n","\n","def test():\n","    root_dir = '/content/dataset_subset_test'\n","    weights_path = '/content/ACMMM23-Solution-MBEG/weights/best_model.pth'\n","\n","    dataloaders, datasets = make_dataloaders(root_dir)\n","    model = DualResNet(num_classes=len(datasets['satellite'].classes))\n","    model.load_state_dict(torch.load(weights_path, map_location='cpu'))\n","    model = model.to('cpu')\n","    model.eval()\n","\n","    gf, gl = extract_features(model, dataloaders['drone'], view=2)\n","    qf, ql = extract_features(model, dataloaders['satellite'], view=1)\n","\n","    CMCs, APs = [], []\n","    for i in range(len(ql)):\n","        ap, cmc = evaluate(qf[i], ql[i], gf, gl)\n","        if cmc[0] == -1:\n","            continue\n","        CMCs.append(cmc.unsqueeze(0))\n","        APs.append(ap)\n","\n","    CMC = torch.cat(CMCs).float().mean(0).numpy()\n","    mAP = np.mean(APs)\n","    print(f\"Recall@1: {CMC[0]*100:.2f}%, Recall@5: {CMC[4]*100:.2f}%, mAP: {mAP*100:.2f}%\")\n","\n","\n","if __name__ == '__main__':\n","    test()\n","\n","'''\n","with open(\"U1652_test_and_evaluate.py\", \"w\") as f:\n","        f.write(code)"],"metadata":{"id":"IxbfJMBCGzEq","executionInfo":{"status":"ok","timestamp":1749333309286,"user_tz":-180,"elapsed":59,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"execution_count":77,"outputs":[]}]}